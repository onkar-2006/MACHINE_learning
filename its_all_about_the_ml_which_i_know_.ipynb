{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6V5b-Sj7StaM"
      },
      "outputs": [],
      "source": [
        "#NOW ITA TIME TO REVISE THE SOME BASIC CONSEPT OF THE ml and dl and then start the ai again\n",
        "\n",
        "\n",
        "\n",
        "import  numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "#load the data\n",
        "data=pd.read_csv(\"data.csv\")\n",
        "X=data.iloc[::-1].values\n",
        "y=data.iloc[:,-1].values"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv(\"file.text\")"
      ],
      "metadata": {
        "id": "JZ8jtPmib3--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pandas some important fuction used to manpulate with the data\n",
        "df.shape\n",
        "df.drop(columns=[\"columns name\"])\n",
        "df.remove(columns=[\"the name of the coumns\"] , inplace=True)\n",
        "df.isin()\n",
        "df.fillna()\n",
        "df.discribe()\n",
        "df.info()\n",
        "df.iloc\n",
        "df.DataFrame\n",
        "df.columns\n",
        "df.index\n",
        "df.dtypes\n",
        "df.head()\n",
        "df.tail()\n",
        "df.isnull()\n",
        "df.isnull().sum()\n",
        "df.dropna()\n",
        "df.isna()\n",
        "df.drop_duplicate()\n",
        "df.rename(columns={\"hari\" : \"sham\"})\n",
        "df.sort_index()\n",
        "df.sort_values()\n",
        "df.apply()\n",
        "df.sample()\n",
        "df[\"columns name\"].values_counts\n",
        "df.applymap(\"columns wisee fuction is apply\")\n",
        "df.to_csv()\n",
        "df.to_excel()"
      ],
      "metadata": {
        "id": "Bbh8AoDHaClI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#seaborn some important ant consept\n",
        "import seaborn as sns\n",
        "sns.catplot(data=df, x=\"columns_name\" ,y=\"columns \" ,hue=\"\" , kind=\"bar\" )\n",
        "sns.relplot(data=df , x=\"columns \" , y=\"columns \",kind=\"scatter\")\n",
        "sns.catplot(data=df , x=\"columns\" , y=\"cplumns\"  ,kind=\"box\")\n",
        "sns.boxplot(df=df , x=\"columns\" , y=\"columns\" )\n",
        "#for the catogorical varible\n",
        "sns.countplot(df=df, x=\"columns\" , y=\"columns\",hue=\"\" , color=\"R\")\n",
        "sns.stripplot(df=df , x=\"col1\" , y=\"col 2\",hue=\"col3\")\n",
        "sns.displot(df=df, x=\"col1\" , y=\"col2\" , hue=\"col3\")\n",
        "sns.rugplot(df =df , x=\"col1\" ,y=\"col2\")\n",
        "sns.kdeplot(df[\"column\"], )\n",
        "sns.histplot(df[\"columns\"] , kde=True)\n",
        "sns.clustermap()\n",
        "sns.rugplot(df['column'])\n",
        "sns.heatmap(df.corr() , annot=True)\n",
        "sns.clustermap(df.corr() , annot=True)\n",
        "sns.pairplot(df)\n",
        "sns.regplot(x=\"col1\" , y=\"col2\")\n",
        "sns.lmplot(df =df , x=\"col1\" , y=\"col2\")\n",
        "sns.residplot(df=df, x=\"column1\" ,y=\"col2\")\n",
        "sns.swarmplot(df=df , x=\"col1\" ,y=\"col2\")\n"
      ],
      "metadata": {
        "id": "6KrY3WutcHRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#split the data into the train and the test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train , X_test , y_train , y_yest=train_test_split(X ,y , test_size=0.2 , random_state=0)"
      ],
      "metadata": {
        "id": "N3QptY7STMzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "si=SimpleImputer(missing_values=np.nan , strategy=\"mean\" ,)\n",
        "X[:,2]=si.fit_transform(X[:,2])"
      ],
      "metadata": {
        "id": "EGX4nOwMVxfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "clusters=KMeans(n_cluster=4 , random_state=42 , verbose=0 )\n",
        "total=clusters.fit(X)\n",
        "total=total.labels_"
      ],
      "metadata": {
        "id": "NivO6DUPWI8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convet the high value into the lower\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc=StandardScaler()\n",
        "X_train_scaled=sc.fit_transform(X_train)\n",
        "X_test_scaled=sc.transfrom(X_test)"
      ],
      "metadata": {
        "id": "MyaD-kybTj3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "sc=MinMaxScaler(feature_range=(0,1))\n",
        "X_train_scaled=sc.fit_transform(X_train)\n",
        "X_test_scaled=sc.transfrom(X_test)"
      ],
      "metadata": {
        "id": "6yuD_d85VlA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#encoding the values\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le=LabelEncoder()\n",
        "X[:,2]=le.fit_transform(X[:,2])"
      ],
      "metadata": {
        "id": "v8Ge8JtRT1DZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "ct=ColumnTransformer(transformers=[\"encoder\" , OneHotEncoder() , [1]] , remainder=\"passthrough\")\n",
        "X_train=ct.fit_transform(X_train)"
      ],
      "metadata": {
        "id": "0-WzXUagUmJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "oe=OrdinalEncoder(categories=[\"hard\" , \"medium\" , \"poor\"]\n"
      ],
      "metadata": {
        "id": "2vWDdCr3Utwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,8))\n",
        "plt.scatter(X[,0] , X[:,1] , c=\"labels\" , c_map=\"viridis\")\n",
        "plt.title(\"printing the clsutering algo\")\n",
        "plt.xlabel(\"feature-1\")\n",
        "plt.ylabel(\"feature-2\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eglL8oA2VLob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca=PCA(n_sample=2)\n",
        "x_scaled=pca.fit_transform(X_scaled)"
      ],
      "metadata": {
        "id": "NUJDUGqwaAuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklern.svm import SVR\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import BayesianRidge\n",
        "from sklearn.linear_model import HuberRegressor\n",
        "from xgboost import XGBRegressor\n",
        "import lightgbm as lgb\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "import lightgbm as lgb\n"
      ],
      "metadata": {
        "id": "YOVKANxrhef0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regression_models = {\n",
        "    'Linear Regression': LinearRegression(),\n",
        "    'Ridge Regression': Ridge(),\n",
        "    'Lasso Regression': Lasso(),\n",
        "    'ElasticNet Regression': ElasticNet(),\n",
        "    'Bayesian Ridge Regression': BayesianRidge(),\n",
        "    'Huber Regressor': HuberRegressor(),\n",
        "    'Decision Tree Regressor': DecisionTreeRegressor(),\n",
        "    'Random Forest Regressor': RandomForestRegressor(),\n",
        "    'SVR (Support Vector Regressor)': SVR(),\n",
        "    'K-Neighbors Regressor': KNeighborsRegressor(),\n",
        "    'AdaBoost Regressor': AdaBoostRegressor(),\n",
        "    'XGBoost Regressor': XGBRegressor(),\n",
        "    'LightGBM Regressor': lgb.LGBMRegressor(),\n",
        "    'Gradient Boosting Regressor': GradientBoostingClassifier()  # Used as regressor\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "id": "lV1mRaQymjs6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of classification models\n",
        "classification_models = {\n",
        "    'Logistic Regression': LogisticRegression(max_iter=200),\n",
        "    'Decision Tree Classifier': DecisionTreeClassifier(),\n",
        "    'Random Forest Classifier': RandomForestClassifier(),\n",
        "    'SVC (Support Vector Classifier)': SVC(),\n",
        "    'K-Neighbors Classifier': KNeighborsClassifier(),\n",
        "    'Naive Bayes': GaussianNB(),\n",
        "    'Linear Discriminant Analysis (LDA)': LinearDiscriminantAnalysis(),\n",
        "    'Quadratic Discriminant Analysis (QDA)': QuadraticDiscriminantAnalysis(),\n",
        "    'Bagging Classifier': BaggingClassifier(),\n",
        "    'Voting Classifier': VotingClassifier(estimators=[\n",
        "        ('lr', LogisticRegression()), ('dt', DecisionTreeClassifier()), ('svc', SVC())\n",
        "    ], voting='hard'),\n",
        "    'Gradient Boosting Classifier': GradientBoostingClassifier(),\n",
        "    'XGBoost Classifier': xgb.XGBClassifier(),\n",
        "    'LightGBM Classifier': lgb.LGBMClassifier()\n",
        "}"
      ],
      "metadata": {
        "id": "N2odHRGEmksv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_regression_model(model, X_train, X_test, y_train, y_test):\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    return mse, r2\n"
      ],
      "metadata": {
        "id": "rJvheaZfmrCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_classification_model(model, X_train, X_test, y_train, y_test):\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    report = classification_report(y_test, y_pred)\n",
        "    return accuracy, report"
      ],
      "metadata": {
        "id": "WnAMbmS9mxtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='weighted')\n",
        "    recall = recall_score(y_test, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')"
      ],
      "metadata": {
        "id": "MXKlqhdMnWFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#some handling transofromers\n",
        "power = PowerTransformer()\n",
        "df['Power Transformed'] = power.fit_transform(df[['Feature']])\n",
        "print(\"\\nPower Transformed Data:\\n\", df.head())\n",
        "\n",
        "function_transformer = FunctionTransformer(lambda x: np.power(x, 2), validate=True)\n",
        "df['Squared'] = function_transformer.fit_transform(df[['Feature']])\n",
        "print(\"\\nSquared Data (Using Function Transformer):\\n\", df.head())\n",
        "\n",
        "function_transformer = FunctionTransformer(lambda x: np.power(x, 2), validate=True)\n",
        "df['Squared'] = function_transformer.fit_transform(df[['Feature']])\n",
        "print(\"\\nSquared Data (Using Function Transformer):\\n\", df.head())\n",
        "\n",
        "\n",
        "\n",
        "#for the outlier\n",
        "plot ->box plot  ,voiline , plot\n",
        "z_socre meethod ,\n",
        "iqr plot , boxplot,persentile method\n",
        "\n",
        "quantile_transformer = QuantileTransformer(n_quantiles=10, output_distribution='uniform')\n",
        "df['Percentile Transformed'] = quantile_transformer.fit_transform(df[['Feature']])\n",
        "print(\"\\nPercentile Transformed Data:\\n\", df.head())\n"
      ],
      "metadata": {
        "id": "Yo0dqs-InW5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import Binarizer, KBinsDiscretizer, QuantileTransformer"
      ],
      "metadata": {
        "id": "i9JVPjlArIGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "binarizer = Binarizer(threshold=0)\n",
        "df['Binarized'] = binarizer.fit_transform(df[['Feature']])\n",
        "print(\"Binarized Data:\\n\", df.head())\n",
        "\n",
        "### 2. Discretization (Binning)\n",
        "# Using KBinsDiscretizer to perform binning\n",
        "kbins = KBinsDiscretizer(n_bins=4, encode='ordinal', strategy='uniform')  # uniform bins\n",
        "df['Discretized'] = kbins.fit_transform(df[['Feature']])\n",
        "print(\"\\nDiscretized Data:\\n\", df.head())\n",
        "\n",
        "### 3. Percentile Transformation\n",
        "# Using QuantileTransformer to scale data based on percentiles\n",
        "percentile_transformer = QuantileTransformer(n_quantiles=10, output_distribution='uniform')\n",
        "df['Percentile Transformed'] = percentile_transformer.fit_transform(df[['Feature']])\n",
        "print(\"\\nPercentile Transformed Data:\\n\", df.head())\n"
      ],
      "metadata": {
        "id": "dHvYskV2rI_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#now the half of the ml is over the next step is the"
      ],
      "metadata": {
        "id": "wn-VK07LrPDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mlxtend.frequent_patterns import apriori, association_rules\n",
        "from mlxtend.preprocessing import TransactionEncoder"
      ],
      "metadata": {
        "id": "SfOCXF7_rlPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from mlxtend.frequent_patterns import apriori, association_rules\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "\n",
        "# Load the dataset (replace with the path to your dataset)\n",
        "df = pd.read_csv('online_retail.csv')\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(df.head())\n",
        "\n",
        "# Preprocess the data\n",
        "# Remove missing 'Description' and canceled transactions (InvoiceNo starting with 'C')\n",
        "df = df[df['Description'].notna()]\n",
        "df = df[~df['InvoiceNo'].str.startswith('C')]\n",
        "\n",
        "# Grouping items by transaction (InvoiceNo)\n",
        "transactions = df.groupby('InvoiceNo')['Description'].apply(list).values.tolist()\n",
        "\n",
        "# Apply TransactionEncoder to one-hot encode the transactions\n",
        "te = TransactionEncoder()\n",
        "te_ary = te.fit(transactions).transform(transactions)\n",
        "df_onehot = pd.DataFrame(te_ary, columns=te.columns_)\n",
        "\n",
        "# Apply Apriori algorithm to find frequent itemsets with a minimum support of 0.01\n",
        "frequent_itemsets = apriori(df_onehot, min_support=0.01, use_colnames=True)\n",
        "\n",
        "# Display the frequent itemsets\n",
        "print(\"\\nFrequent Itemsets:\")\n",
        "print(frequent_itemsets)\n",
        "\n",
        "# Generate association rules with a minimum lift of 1.0\n",
        "rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1.0)\n",
        "\n",
        "# Display the association rules\n",
        "print(\"\\nAssociation Rules:\")\n",
        "print(rules)\n",
        "\n",
        "# Filter rules with confidence >= 0.7\n",
        "filtered_rules = rules[rules['confidence'] >= 0.7]\n",
        "\n",
        "# Display the filtered rules\n",
        "print(\"\\nFiltered Association Rules (Confidence >= 0.7):\")\n",
        "print(filtered_rules)\n"
      ],
      "metadata": {
        "id": "pcYVECfGsFvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#now its time to satrt the nlp that is the natural language processing\n",
        "the preprocessing for the nlp thst is the basic text cleaning\n",
        "\n",
        "\n",
        "import nltk\n",
        "import re\n"
      ],
      "metadata": {
        "id": "yJIXAF1GsKz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def html_tag_removeal(text):\n",
        "  pattern=re.compile('<.*?>#')\n",
        "  return pattern.sub(r'' , text)\n",
        "data[\"safe_text\"]=data[\"safe_text\"].apply(html_tag_removal)"
      ],
      "metadata": {
        "id": "gXw20Hc1uAE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_url(text):\n",
        "  pattern=re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "  return pattern.sub(r'' , text)\n",
        ""
      ],
      "metadata": {
        "id": "iahqTg11uZwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data[\"safe_text\"]=data[\"safe_text\"].apply(remove_url)"
      ],
      "metadata": {
        "id": "KKLwbTUxuqOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove the punkt\n",
        "exclude=string.punctuation\n",
        "def remove_punc1(text):\n",
        "    return text.translate(str.maketrans('', '', exclude))\n"
      ],
      "metadata": {
        "id": "gIHuArIGuq1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "def remove_stopword(text):\n",
        "  next_text=[]\n",
        "  for word in text.split():\n",
        "    if word in stopwords.words(\"english\"):\n",
        "      new_text.append('')\n",
        "    else:\n",
        "      new_text.append(word)\n",
        ""
      ],
      "metadata": {
        "id": "_DaQkPzYu8yN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove the stopwords\n",
        "from nltk.corpus import stopwords\n",
        "def remove_stopwords(text):\n",
        "    new_text = []\n",
        "\n",
        "    for word in text.split():\n",
        "        if word in stopwords.words('english'):\n",
        "            new_text.append('')\n",
        "        else:\n",
        "            new_text.append(word)\n",
        "    x = new_text[:]\n",
        "    new_text.clear()\n",
        "    return \" \".join(x)\n",
        "\n"
      ],
      "metadata": {
        "id": "rILBU_TUvaeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "convert to lower\n",
        "data[\"safe_text\"]=data[\"safe_text\"].str.lower()"
      ],
      "metadata": {
        "id": "MrQ7YvlEvhGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "pIDYI-G3vlBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "remove the emojis\n",
        "def remove_emoji(text):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)\n",
        "\n",
        "\n",
        "data[\"safe_text\"]=data[\"safe_text\"].apply(remove_emoji)\n",
        ""
      ],
      "metadata": {
        "id": "TbXBc_4NvuWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stem is nothing but the original word"
      ],
      "metadata": {
        "id": "rz2L9bYKvxed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#the lamitizeer is the nothing but conect in to the actual word by remving the ing and otheree"
      ],
      "metadata": {
        "id": "tvfS6dE2v-ac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "def simple_lemmatizer(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = nltk.word_tokenize(text)\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    return ' '.join(lemmatized_words)\n"
      ],
      "metadata": {
        "id": "oL_YLeB8wKoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "import os"
      ],
      "metadata": {
        "id": "H9rLeIU2wLUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = gensim.models.Word2Vec(\n",
        "    window=10,\n",
        "    min_count=2\n",
        ")"
      ],
      "metadata": {
        "id": "0SDUHfMuwQws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Sample text corpus\n",
        "sentences = [\n",
        "    [\"i\", \"love\", \"machine\", \"learning\"],\n",
        "    [\"deep\", \"learning\", \"is\", \"great\"],\n",
        "    [\"word\", \"embeddings\", \"are\", \"powerful\"],\n",
        "]\n",
        "\n",
        "# Train Word2Vec model\n",
        "model = Word2Vec(sentences, min_count=1, vector_size=10, window=5)\n",
        "\n",
        "# Get the vector for a word\n",
        "word_vector = model.wv['learning']\n",
        "\n",
        "print(\"Vector for 'learning':\")\n",
        "print(word_vector)"
      ],
      "metadata": {
        "id": "DFGIC1ibwhov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Sample documents (corpus)\n",
        "documents = [\n",
        "    \"I love machine learning\",\n",
        "    \"Deep learning is great\",\n",
        "    \"Word embeddings are powerful\",\n",
        "]\n",
        "\n",
        "# Create the TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the documents into TF-IDF vectors\n",
        "tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Convert to dense matrix and display\n",
        "dense_matrix = tfidf_matrix.todense()\n",
        "\n",
        "# Show the TF-IDF vectors for each word\n",
        "print(\"TF-IDF Vectors (words as columns):\")\n",
        "df_tfidf = pd.DataFrame(dense_matrix, columns=vectorizer.get_feature_names_out())\n",
        "print(df_tfidf)"
      ],
      "metadata": {
        "id": "KI0NcX5bwiY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Load GloVe embeddings (assuming 'glove.6B.50d.txt' file is in the same directory)\n",
        "def load_glove_embeddings(file_path):\n",
        "    embeddings = {}\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings[word] = vector\n",
        "    return embeddings\n",
        "\n",
        "# Load GloVe embeddings\n",
        "glove_embeddings = load_glove_embeddings('glove.6B.50d.txt')\n",
        "\n",
        "# Retrieve vector for a word\n",
        "word_vector = glove_embeddings.get(\"learning\")\n",
        "\n",
        "print(\"Vector for 'learning':\")\n",
        "print(word_vector)"
      ],
      "metadata": {
        "id": "qLqqPf6JwsF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PQrSlNKNwt_H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}